---
---

@INPROCEEDINGS{10467665,
  author={M R, Akhil and Krishna V Sharma, Adithya and Nadig, Abhijith and A, Pavan and Shetty, Ashray and A, Sumathi},
  booktitle={2024 International Conference on Intelligent and Innovative Technologies in Computing, Electrical and Electronics (IITCEE)}, 
  title={Deep Learning for Cyberthreats: Performance Analysis and Application of Malware Classification in Edge Computing}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  keywords={Performance evaluation;Image edge detection;Computational modeling;Biological system modeling;Artificial neural networks;Computer architecture;Malware;Cybersecurity;Data Protection;Deep Neural Networks;IoT Security;Malware Classification;Performance Evaluation},
  doi={10.1109/IITCEE59897.2024.10467665},
  abstract={Approximately one among seven people in the world experience some form of mental health disorder. Listening to music has been found to substantially improve mental health via stress-reducing effects. A 2009 meta-analysis found that music-assisted relaxation can also improve the quality of sleep in patients with sleep disorders. In this paper we present DeepTunes, a system to generate music with lyrics, according to the detected emotion of the user. The user needs to submit a photo of himself for the Facial Recognition model to determine the user’s mood and provide some additional textual input describing how the user feels. The lyric generation model takes into consideration the mood of the user as determined by the Facial Recognition model and generates the lyrics accordingly with the first line provided by the user. Background music will also be generated that best suits the user’s mood. Deep Learning Techniques have been employed for Facial Recognition, Lyrics Generation and Music Generation models. For the facial recognition model, a Convolutional Neural Network (CNN) has been implemented, GPT-2 has been used to generate lyrics and stacked LSTM networks have been constructed to generate music. The goal of our study is to produce music and lyrics that generate a positive emotive response from the user, given any mood.},
  bibtex_show = {true},
  html={https://ieeexplore.ieee.org/abstract/document/10467665},
  abbr={IITCEE-2024}
}

@INPROCEEDINGS{9825153,
  author={P, Vishesh and A, Pavan and Vasist, Samarth G and Rao, Sindhu and Srinivas, K. S.},
  booktitle={2022 IEEE 7th International conference for Convergence in Technology (I2CT)}, 
  title={DeepTunes - Music Generation based on Facial Emotions using Deep Learning}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  keywords={Deep learning;Emotion recognition;Mood;Face recognition;Conferences;Music;Mental health;Deep Learning;LSTM;CNN;GPT-2;sentiment;Music generation;Lyrics generation;Emotion Recognition},
  doi={10.1109/I2CT54291.2022.9825153},
  abstract={Approximately one among seven people in the world experience some form of mental health disorder. Listening to music has been found to substantially improve mental health via stress-reducing effects. A 2009 meta-analysis found that music-assisted relaxation can also improve the quality of sleep in patients with sleep disorders. In this paper we present DeepTunes, a system to generate music with lyrics, according to the detected emotion of the user. The user needs to submit a photo of himself for the Facial Recognition model to determine the user’s mood and provide some additional textual input describing how the user feels. The lyric generation model takes into consideration the mood of the user as determined by the Facial Recognition model and generates the lyrics accordingly with the first line provided by the user. Background music will also be generated that best suits the user’s mood. Deep Learning Techniques have been employed for Facial Recognition, Lyrics Generation and Music Generation models. For the facial recognition model, a Convolutional Neural Network (CNN) has been implemented, GPT-2 has been used to generate lyrics and stacked LSTM networks have been constructed to generate music. The goal of our study is to produce music and lyrics that generate a positive emotive response from the user, given any mood.},
  bibtex_show = {true},
  html={https://ieeexplore.ieee.org/abstract/document/9825153},
  preview       = {deeptunes.png},
  abbr={I2CT-2022}
}

@article{DBLP:journals/corr/abs-2008-10325,
  author       = {Pavan A and
                  Adithya Bennur and
                  Mohit Gaggar and
                  Shylaja S. S},
  title        = {LCA-Net: Light Convolutional Autoencoder for Image Dehazing},
  journal      = {CoRR},
  volume       = {abs/2008.10325},
  year         = {2020},
  url          = {https://arxiv.org/abs/2008.10325},
  eprinttype    = {arXiv},
  eprint       = {2008.10325},
  timestamp    = {Fri, 28 Aug 2020 12:11:44 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2008-10325.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract     = {Image dehazing is a crucial image pre-processing task aimed at removing the incoherent noise generated by haze to improve the visual appeal of the image. The existing models use sophisticated networks and custom loss functions which are computationally inefficient and requires heavy hardware to run. Time is of the essence in image pre-processing since real time outputs can be obtained instantly. To overcome these problems, our proposed generic model uses a very light convolutional encoder-decoder network which does not depend on any atmospheric models. The network complexity-image quality trade off is handled well in this neural network and the performance of this network is not limited by low-spec systems. This network achieves optimum dehazing performance at a much faster rate, on several standard datasets, comparable to the state-of-the-art methods in terms of image quality.},
  bibtex_show = {true},
  pdf           = {https://arxiv.org/pdf/2008.10325},
  html          = {https://arxiv.org/abs/2008.10325},
  preview       = {lca-net.png},
  abbr={ARXIV-2020}
}
